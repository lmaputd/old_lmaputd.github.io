[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language Model and Analytics Project (LMAP)",
    "section": "",
    "text": "Mission:\n\nTo serve as a hub for researchers from different disciplines to collaborate on data science projects, including large language and computational models.\nTo facilitate the development and application of state-of-the-art techniques in novel language models involving social and text data.\nTo encourage interdisciplinary research that synergizes social science, computer science, biomedical, and neuroscience studies.\nTo create a platform for sharing knowledge, ideas, and resources among members.\n\n\n\n\n\n\n\n\n\n\n\nLMAP: word2vec\n\n\n\n\n\n\n\nlanguage model\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nKarl Ho.\n\n\n\n\n\n\n\n\nLMAP: What is Language Model?\n\n\n\n\n\n\n\nlanguage model\n\n\nmethods\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nKarl Ho.\n\n\n\n\n\n\n\n\nLMAP Meeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nLMAP admin.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "LMAP Meeting",
    "section": "",
    "text": "LMAP meets every Friday for project presentations and planning.\nTime: 11:00 am -12:30 pm\nLocation: Permian Basin Conference Room, Green Hall"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Introduction\n\nThe “Language Model and Analytics Project” is an interdisciplinary research initiative that aims to explore, develop, and apply advanced language models and data analytics techniques to a wide range of problems across various domains. By bringing together researchers from diverse disciplines, this project seeks to foster collaboration and innovation in the study and application of large language models and data-driven methods.\n\nScope:\n\nThe scope of the project encompasses the following areas:\nLarge Language Models: Investigating state-of-the-art language models, such as GPT-like architectures, BERT, and transformers, as well as exploring novel techniques for developing and improving language models.\nData Analytics: Applying advanced data science methods, including machine learning, deep learning, and natural language processing, to analyze and make sense of large-scale, unstructured text data.\nInterdisciplinary Research: Bridging the gap between disciplines, such as social sciences, computer science, biology, and neuroscience, to address complex problems that require a diverse set of expertise and approaches.\nPractical Applications: Developing and implementing real-world solutions using language models and analytics techniques across various domains, such as healthcare, finance, marketing, policy-making, and more.\n\nResearch Direction:\n\nThe research direction for the Language Model and Analytics Project can be divided into three main pillars:\n\nFundamental Research: Investigate the underlying principles, algorithms, and techniques that drive large language models and data analytics. This includes exploring new methods for training, fine-tuning, and evaluating language models, as well as understanding their strengths and limitations.\nApplied Research: Apply language models and analytics techniques to solve specific problems in various domains. This may involve designing novel applications, refining existing methods, or analyzing the impact of these techniques on society, economy, and policy.\nCross-disciplinary Collaboration: Encourage collaboration and knowledge exchange among researchers from different disciplines to inspire new perspectives and ideas. This can lead to the development of innovative solutions that integrate the expertise of multiple fields."
  },
  {
    "objectID": "posts/whatislanguagemodel/index.html",
    "href": "posts/whatislanguagemodel/index.html",
    "title": "LMAP: What is Language Model?",
    "section": "",
    "text": "What is language model?\n\nUnderstanding the Basics\n\nA language model is a type of artificial intelligence (AI) algorithm that can learn the patterns and structure of language, allowing it to generate new text that is similar to human writing. In other words, language models are machines that can understand and create language.\nLanguage models have been around for decades, but recent advances in AI and machine learning have led to significant improvements in their accuracy and sophistication. Today, some of the most powerful language models are based on deep learning techniques such as neural networks, which allow them to process vast amounts of data and learn complex patterns in language.\nSo what can language models be used for? In political and social studies, language models have a wide range of applications. For example:\n- Sentiment analysis: Language models can be used to analyze large volumes of text data from social media, news articles, and other sources to determine the overall sentiment or emotional tone of the text. This can be useful for understanding public opinion on political issues or tracking changes in sentiment over time.\n- Political document analysis: Language models can be used to analyze political documents such as speeches, policy statements, and party platforms to identify key themes and topics, track changes in political discourse over time, and detect biases or inconsistencies in the text.\n- Election forecasting and modeling: Language models can be used to predict the outcomes of elections based on factors such as candidate speeches, polling data, and social media activity. This can help political campaigns and analysts to make more informed decisions about strategy and messaging (Gayo-Avello 2012, Tumasjan 2010).\n- Social network analysis of political actors: Language models can be used to analyze social network data to identify key political actors and their connections, track the flow of information through the network, and detect patterns of influence and power.\n- Automated content analysis of political speeches and debates: Language models can be used to automatically classify political speeches and debates based on their content, identifying key themes, topics, and arguments. This can be useful for tracking changes in political discourse over time, comparing the positions of different candidates, and identifying areas of consensus or disagreement (e.g. polarization in Adamic and Glance. 2005.)\n- Detection of fake news and disinformation: Language models can be used to identify and flag misleading or false information in news articles, social media posts, and other sources. This can help to combat the spread of misinformation and promote more accurate reporting.\nThese are just a few examples of the ways in which language models can be used in political and social studies. As the technology continues to evolve, we can expect to see even more innovative applications in the future.\nOne notable example of a language model that has had a significant impact on political and social studies is the GPT-3 (Generative Pre-trained Transformer 3) language model. GPT-3 is a highly advanced deep learning model developed by OpenAI, trained on a massive corpus of text data to generate highly coherent and contextually relevant text. Its capabilities include natural language processing (NLP), question-answering, language translation, and even creative writing.\nIn addition to GPT-3, there are many other language models and techniques that are being used in political and social studies, including BERT (Bidirectional Encoder Representations from Transformers), ELMo (Embeddings from Language Models), and ULMFiT (Universal Language Model Fine-tuning), among others. Each of these models has its own strengths and weaknesses, and can be applied in a variety of contexts to help researchers gain new insights into language and social behavior.\nAnother example is the use of topic modeling techniques to identify key issues and themes in political discourse. Topic modeling is a type of unsupervised machine learning that can automatically identify and extract topics from large text datasets. Researchers can use these topics to identify patterns and trends in political discussions and debates, and to track changes in public opinion over time.\nIn addition to these examples, there are many other ways that natural language processing and machine learning techniques can be applied in political and social studies. By analyzing large text datasets and identifying patterns and trends in language use and social behavior, researchers can gain new insights into the complex social and political issues facing our world today.\nProbability in a language model:\n\\[\nP(w_1^n) = \\prod_{i=1}^n P(w_i|w_1^{i-1})\n\\tag{1}\\]\nThis formula states that the probability of a sequence of words \\(w_1^n\\) is equal to the product of the conditional probabilities of each word given its preceding words in the sequence.\nThe top five most influential works/papers are as follows:\n1. “Efficient Estimation of Word Representations in Vector Space” (Mikolov et al. 2013) by Tomas Mikolov et al. This paper introduced the word2vec algorithm, which revolutionized the way that word embeddings are trained and used.\n2. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(Devlin et al. 2019)” by Jacob Devlin et al. This paper introduced the BERT model, which achieved state-of-the-art results on a wide range of natural language processing tasks.\n3. “Language Models are Few-Shot Learners” by Tom Brown et al. “(Brown et al. 2020)This paper demonstrated the impressive few-shot learning capabilities of the GPT-3 language model, which can generate coherent and grammatical text with minimal task-specific training.\n4. “Attention Is All You Need” by Ashish Vaswani et al. (Vaswani et al. 2017) This paper introduced the Transformer architecture, which is now the dominant approach for sequence modeling tasks in natural language processing.\n5. “Deep contextualized word representations” by Matthew Peters et al. (Peters et al. 1802) This paper introduced the ELMo model, which was the first to demonstrate the power of deep contextualized embeddings for natural language understanding tasks.\nThe impacts of these papers have been significant in shaping the direction of research in language modeling. They have led to the development of new models that achieve state-of-the-art results on a wide range of natural language processing tasks, including machine translation, language modeling, and sentiment analysis. They have also sparked interest in transfer learning and few-shot learning, and have led to the development of large-scale pre-training datasets and models that have greatly accelerated progress in the field.\nIn conclusion, language models are a powerful tool for political and social scientists, providing new ways to analyze and understand language, culture, and social behavior. As these models continue to evolve and improve, they hold tremendous promise for advancing our understanding of the complex social and political issues facing our world today.\n\n\n\n\nReferences\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In, 33:18771901. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” May. https://doi.org/10.48550/arXiv.1810.04805.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space,” September. https://doi.org/10.48550/arXiv.1301.3781.\n\n\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 1802. “Deep Contextualized Word Representations. CoRR Abs/1802.05365 (2018).” arXiv Preprint arXiv:1802.05365.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Karl Ho\n\n\nResearch interest: Large Language Models in political studies, models on social data, elections, East Asia, particularly Taiwan and Hong Kong,\nCurrent projects: Hyperpartisanship, Politics of culture war"
  },
  {
    "objectID": "posts/word2vec/index.html",
    "href": "posts/word2vec/index.html",
    "title": "LMAP: word2vec",
    "section": "",
    "text": "What is word2vec?\nWord2vec(Mikolov et al. 2013a) is a popular word embedding algorithm introduced by Tomas Mikolov et al. in 2013. It was developed at Google (“Google Code Archive - Long-Term Storage for Google Code Project Hosting.” n.d.)as a method to learn high-quality distributed representations of words from large amounts of unstructured text data.\nDescription:\nWord2vec employs a shallow neural network with either the continuous bag-of-words (CBOW) or skip-gram architecture. CBOW predicts a target word based on its context words, while skip-gram predicts context words given a target word. The training process adjusts the word vector representations to maximize the likelihood of correctly predicting the surrounding words.\n1. Bag-of-Words (CBOW):\nDefinition: The bag-of-words (CBOW) model is a shallow neural network architecture used for generating word embeddings. It predicts a target word based on the context words surrounding it.\nFormula (CBOW):\n$$\n$$\n\\[\n\\underset{\\text{context words}}{\\text{Input}} \\rightarrow \\underset{\\text{target word}}{\\text{Output}}\n\\]\n2. Skip-Gram Architecture:\nDefinition: The skip-gram architecture is another shallow neural network architecture used for learning word embeddings. It predicts context words given a target word.\nFormula (Skip-Gram):\n\\[\n\\underset{\\text{target word}}{\\text{Input}} \\rightarrow \\underset{\\text{context words}}{\\text{Output}}\n\\]\nNotable Citations:\n1. “Efficient Estimation of Word Representations in Vector Space” by Tomas Mikolov et al. (2013)(Mikolov et al. 2013b): This is the original paper introducing word2vec. It provides a detailed description of the model and its training algorithms.\n2. “Distributed Representations of Words and Phrases and their Compositionality” by Tomas Mikolov et al. (2013): This paper expands on the word2vec model, presenting the skip-gram variant and exploring the compositionality of word vectors.\nApplications in Social and Political Studies:\nWord2vec has been widely applied in social and political studies, enabling researchers to gain insights into language use, sentiment analysis, and political discourse. Here are a few notable applications:\n\nElection campaign using social media[Jungherr (2016)](Chauhan, Sharma, and Sikka 2021)\nHate speech or toxic comment classification(Saeed et al. 2021; Rizos, Hemker, and Schuller 2019):\nClustering/classification of large document datasets(Park et al. 2019)\nAdvanced sentiment analysis using combined features and ensemble learning(Park et al. 2019)\n\nMore to come.\n\n\n\n\nReferences\n\nChauhan, Priyavrat, Nonita Sharma, and Geeta Sikka. 2021. “The Emergence of Social Media Data and Sentiment Analysis in Election Prediction.” Journal of Ambient Intelligence and Humanized Computing 12 (2): 2601–27. https://doi.org/10.1007/s12652-020-02423-y.\n\n\n“Google Code Archive - Long-Term Storage for Google Code Project Hosting.” n.d. https://code.google.com/archive/p/word2vec/.\n\n\nJungherr, Andreas. 2016. “Twitter Use in Election Campaigns: A Systematic Literature Review.” Journal of Information Technology & Politics 13 (1): 72–91. https://doi.org/10.1080/19331681.2015.1132401.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. “Efficient Estimation of Word Representations in Vector Space,” September. https://doi.org/10.48550/arXiv.1301.3781.\n\n\n———. 2013b. “Efficient Estimation of Word Representations in Vector Space,” September. https://doi.org/10.48550/arXiv.1301.3781.\n\n\nPark, Jinuk, Chanhee Park, Jeongwoo Kim, Minsoo Cho, and Sanghyun Park. 2019. “ADC: Advanced Document Clustering Using Contextualized Representations.” Expert Systems with Applications 137 (December): 157–66. https://doi.org/10.1016/j.eswa.2019.06.068.\n\n\nRizos, Georgios, Konstantin Hemker, and Björn Schuller. 2019. “Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate-Speech Classification.” In, 9911000. CIKM ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3357384.3358040.\n\n\nSaeed, Hafiz Hassaan, Muhammad Haseeb Ashraf, Faisal Kamiran, Asim Karim, and Toon Calders. 2021. “Roman Urdu Toxic Comment Classification.” Language Resources and Evaluation 55 (4): 971–96. https://doi.org/10.1007/s10579-021-09530-y."
  }
]