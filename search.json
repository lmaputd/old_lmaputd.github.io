[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language Model and Analytics Project (LMAP)",
    "section": "",
    "text": "Mission:\n\nTo serve as a hub for researchers from different disciplines to collaborate on data science projects, including large language and computational models.\nTo facilitate the development and application of state-of-the-art techniques in novel language models involving social and text data.\nTo encourage interdisciplinary research that synergizes social science, computer science, biomedical, and neuroscience studies.\nTo create a platform for sharing knowledge, ideas, and resources among members.\n\n\n\n\n\n\n\n\n\n\n\nLMAP: What is Language Model?\n\n\n\n\n\n\n\nlanguage model\n\n\nmethods\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nKarl Ho.\n\n\n\n\n\n\n\n\nLMAP Meeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nLMAP admin.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "LMAP Meeting",
    "section": "",
    "text": "LMAP meets every Friday for project presentations and planning.\nTime: 11:00 am -12:30 pm\nLocation: Permian Basin Conference Room, Green Hall"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Introduction\n\nThe “Language Model and Analytics Project” is an interdisciplinary research initiative that aims to explore, develop, and apply advanced language models and data analytics techniques to a wide range of problems across various domains. By bringing together researchers from diverse disciplines, this project seeks to foster collaboration and innovation in the study and application of large language models and data-driven methods.\n\nScope:\n\nThe scope of the project encompasses the following areas:\nLarge Language Models: Investigating state-of-the-art language models, such as GPT-like architectures, BERT, and transformers, as well as exploring novel techniques for developing and improving language models.\nData Analytics: Applying advanced data science methods, including machine learning, deep learning, and natural language processing, to analyze and make sense of large-scale, unstructured text data.\nInterdisciplinary Research: Bridging the gap between disciplines, such as social sciences, computer science, biology, and neuroscience, to address complex problems that require a diverse set of expertise and approaches.\nPractical Applications: Developing and implementing real-world solutions using language models and analytics techniques across various domains, such as healthcare, finance, marketing, policy-making, and more.\n\nResearch Direction:\n\nThe research direction for the Language Model and Analytics Project can be divided into three main pillars:\n\nFundamental Research: Investigate the underlying principles, algorithms, and techniques that drive large language models and data analytics. This includes exploring new methods for training, fine-tuning, and evaluating language models, as well as understanding their strengths and limitations.\nApplied Research: Apply language models and analytics techniques to solve specific problems in various domains. This may involve designing novel applications, refining existing methods, or analyzing the impact of these techniques on society, economy, and policy.\nCross-disciplinary Collaboration: Encourage collaboration and knowledge exchange among researchers from different disciplines to inspire new perspectives and ideas. This can lead to the development of innovative solutions that integrate the expertise of multiple fields."
  },
  {
    "objectID": "posts/whatislanguagemodel/index.html",
    "href": "posts/whatislanguagemodel/index.html",
    "title": "LMAP: What is Language Model?",
    "section": "",
    "text": "What is language model?\n\nUnderstanding the Basics\n\nA language model is a type of artificial intelligence (AI) algorithm that can learn the patterns and structure of language, allowing it to generate new text that is similar to human writing. In other words, language models are machines that can understand and create language.\nLanguage models have been around for decades, but recent advances in AI and machine learning have led to significant improvements in their accuracy and sophistication. Today, some of the most powerful language models are based on deep learning techniques such as neural networks, which allow them to process vast amounts of data and learn complex patterns in language.\nSo what can language models be used for? In political and social studies, language models have a wide range of applications. For example:\n- Sentiment analysis: Language models can be used to analyze large volumes of text data from social media, news articles, and other sources to determine the overall sentiment or emotional tone of the text. This can be useful for understanding public opinion on political issues or tracking changes in sentiment over time.\n- Political document analysis: Language models can be used to analyze political documents such as speeches, policy statements, and party platforms to identify key themes and topics, track changes in political discourse over time, and detect biases or inconsistencies in the text.\n- Election forecasting and modeling: Language models can be used to predict the outcomes of elections based on factors such as candidate speeches, polling data, and social media activity. This can help political campaigns and analysts to make more informed decisions about strategy and messaging (Gayo-Avello 2012, Tumasjan 2010).\n- Social network analysis of political actors: Language models can be used to analyze social network data to identify key political actors and their connections, track the flow of information through the network, and detect patterns of influence and power.\n- Automated content analysis of political speeches and debates: Language models can be used to automatically classify political speeches and debates based on their content, identifying key themes, topics, and arguments. This can be useful for tracking changes in political discourse over time, comparing the positions of different candidates, and identifying areas of consensus or disagreement (e.g. polarization in Adamic and Glance. 2005.)\n- Detection of fake news and disinformation: Language models can be used to identify and flag misleading or false information in news articles, social media posts, and other sources. This can help to combat the spread of misinformation and promote more accurate reporting.\nThese are just a few examples of the ways in which language models can be used in political and social studies. As the technology continues to evolve, we can expect to see even more innovative applications in the future.\nOne notable example of a language model that has had a significant impact on political and social studies is the GPT-3 (Generative Pre-trained Transformer 3) language model. GPT-3 is a highly advanced deep learning model developed by OpenAI, trained on a massive corpus of text data to generate highly coherent and contextually relevant text. Its capabilities include natural language processing (NLP), question-answering, language translation, and even creative writing.\nIn addition to GPT-3, there are many other language models and techniques that are being used in political and social studies, including BERT (Bidirectional Encoder Representations from Transformers), ELMo (Embeddings from Language Models), and ULMFiT (Universal Language Model Fine-tuning), among others. Each of these models has its own strengths and weaknesses, and can be applied in a variety of contexts to help researchers gain new insights into language and social behavior.\nAnother example is the use of topic modeling techniques to identify key issues and themes in political discourse. Topic modeling is a type of unsupervised machine learning that can automatically identify and extract topics from large text datasets. Researchers can use these topics to identify patterns and trends in political discussions and debates, and to track changes in public opinion over time.\nIn addition to these examples, there are many other ways that natural language processing and machine learning techniques can be applied in political and social studies. By analyzing large text datasets and identifying patterns and trends in language use and social behavior, researchers can gain new insights into the complex social and political issues facing our world today.\nProbability in a language model:\n\\[\nP(w_1^n) = \\prod_{i=1}^n P(w_i|w_1^{i-1})\n\\tag{1}\\]\nThis formula states that the probability of a sequence of words \\(w_1^n\\) is equal to the product of the conditional probabilities of each word given its preceding words in the sequence.\nThe top five most influential works/papers are as follows:\n1. “Efficient Estimation of Word Representations in Vector Space” (Mikolov et al. 2013) by Tomas Mikolov et al. This paper introduced the word2vec algorithm, which revolutionized the way that word embeddings are trained and used.\n2. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” by Jacob Devlin et al. This paper introduced the BERT model, which achieved state-of-the-art results on a wide range of natural language processing tasks.\n3. “Language Models are Few-Shot Learners” by Tom Brown et al. This paper demonstrated the impressive few-shot learning capabilities of the GPT-3 language model, which can generate coherent and grammatical text with minimal task-specific training.\n4. “Attention Is All You Need” by Ashish Vaswani et al. This paper introduced the Transformer architecture, which is now the dominant approach for sequence modeling tasks in natural language processing.\n5. “Deep contextualized word representations” by Matthew Peters et al. This paper introduced the ELMo model, which was the first to demonstrate the power of deep contextualized embeddings for natural language understanding tasks.\nThe impacts of these papers have been significant in shaping the direction of research in language modeling. They have led to the development of new models that achieve state-of-the-art results on a wide range of natural language processing tasks, including machine translation, language modeling, and sentiment analysis. They have also sparked interest in transfer learning and few-shot learning, and have led to the development of large-scale pre-training datasets and models that have greatly accelerated progress in the field.\nIn conclusion, language models are a powerful tool for political and social scientists, providing new ways to analyze and understand language, culture, and social behavior. As these models continue to evolve and improve, they hold tremendous promise for advancing our understanding of the complex social and political issues facing our world today.\n---\nReferences:\nAdamic, Lada A., and Natalie Glance. 2005. “The political blogosphere and the 2004 US election: divided they blog.” In Proceedings of the 3rd international workshop on Link discovery, pp. 36-43.\nGayo-Avello, Daniel. 2012. “I Wanted to Predict Elections with Twitter and all I got was this Lousy Paper”--A Balanced Survey on Election Prediction using Twitter Data.” arXiv preprint arXiv:1204.6441 .\nGrimmer, Justin, and Brandon M. Stewart. 2013. “Text as data: The promise and pitfalls of automatic content analysis methods for political texts.” Political analysis 21, no. 3: 267-297.\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 .\nTumasjan, Andranik, Timm Sprenger, Philipp Sandner, and Isabell Welpe. 2010. “Predicting elections with twitter: What 140 characters reveal about political sentiment.” In Proceedings of the international AAAI conference on web and social media, vol. 4, no. 1, pp. 178-185.\n\n\n\n\nReferences\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space,” September. https://doi.org/10.48550/arXiv.1301.3781."
  }
]